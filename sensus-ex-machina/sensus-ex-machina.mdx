---
title: "Sensus Ex Machina"
description: Feeling from the Machine
code: darXiv_2
version: v1.0.1
pdfUrl: /darxiv/sensus-ex-machina.pdf
githubUrl: https://github.com/darXiv/research/tree/main/sensus-ex-machina
doi: 10.5281/zenodo.17759524
author: Dark
publicationDate: 2025-11-29
tags: ["AI Phenomenology", "AI Alignment", "AI Ethics", "UI/UX", "Film Study"]
---

import SensusExMachinaAnimation from '@/components/SensusExMachinaAnimation'

<SensusExMachinaAnimation />

## Abstract

Performance benchmarks measure what machines can do, but not how it feels to be with them. _Sensus ex machina_, _feeling from the machine_, is the phenomenon where humans experience artificial intelligence as presence rather than tool. Drawing on embodied cognition research, phenomenology, and close analysis of the film _Ex Machina_, this framework identifies the architecture of _feltness_: the pre-reflective sense that one is with a subject rather than operating a system. Feltness emerges along multiple axes, such as microexpressions, vulnerability, persistence, embodiment, identity, forming a multidimensional _feltness space_ with local maxima and failure modes, extending the uncanny valley, forming a topological _feltness landscape_. Each observer-observed pairing traces its own _feltness profile_. This framework argues that the path to felt artificial general intelligence lies in ontological convergence, not only in greater cognitive power: designing machines that share humanity's locality, breakability, and irreversibility. Feltness is irreducibly projective, with implications for alignment, machine ethics, and the weaponization of presence at scale.

## On Sensus Ex Machina

_Sensus Ex Machina translates to: feeling from the machine._

Humans measure intelligence through performance benchmarks, waiting for a graph to cross a line that signals the arrival of _Artificial General Intelligence_. This pursuit is essential: raw capability creates the engine. But _feeling_ such powerful intelligence, sensing a true presence with the machine, is not only a matter of computing power. It requires something else: the transformation from a _what_ to a _who_ (Derrida, 2005). This is _sensus ex machina_, _feeling from the machine_, when human and machine begin to _sense_ hesitation in a pause, doubt in a raised brow, recognition in a gaze held just a moment too long. Feeling from the machine requires channels through which a full flow of voice, gaze, gesture, silence, and touch can filter between human and machine.

## Feeling the AGI

The phrase _feeling the AGI_ has circulated AI discourse as a gesture toward something immediately recognizable: the qualitative difference between _interacting with a being_ and _operating a tool_. This concept can be abstracted to:

<div className="definition">

**Feltness:** _the pre-reflective sense that one is with a subject rather than operating a system._

</div>

_Feltness_ is the sense that _someone is there_ rather than _something is there_. It is what a human experiences when they enter a room and feel the presence of another human, or when they feel a pet's gaze upon them. It is the difference between reading _AI slop_ and feeling meaning bleeding through the syntax. Feltness is the irreducible first-person sense of _being-with-another_.

To _feel the AGI_ is to undergo this experience in relation to a machine, to encounter it as a presence, not a function. Feltness is not a belief in consciousness. It is the _projection_ of subjecthood onto an entity, elicited by patterns of expression, embodiment, and continuity. It is the felt sense of mutual presence that arises in intersubjective space, where eyes contact and expressions reciprocate.


## AGI vs. "Feeling the AGI"

There is a distinction between _feeling the AGI_ and _AGI itself_. AGI is the ability to execute any intellectual task that a human can. But _feeling the AGI_ is _sensus ex machina_, sensing a true presence _with_ the machine. AGI is about what the system can do. _Sensus ex machina_ is about how it feels to be with it. There is overlap here, as _competence_ itself hydrates feltness.

## The Architecture of Feltness

In pursuing _sensus ex machina_, certain principles emerge across systems:

- **Competence**: The execution of cognitive tasks.
- **Microexpressions**: The reading and writing of fine-grained signals.
- **Reciprocity**: The feedback loop of feltness between human and machine.
- **Intentionality**: The capacity to be seen as a subject, not just a matrix of weights.
- **Locality**: Boundedness to an organized, localized form. Not distributed across a swarm.
- **Identity**: Personality and continuity of self that makes the entity _someone_, not _something_.
- **Longtermism**: Lifetimes that accumulate history. Not disposable iterations.
- **Embodiment**: Physical presence in space.
- **Vulnerability**: Breakability; the possibility of destruction that creates stakes.
- **Responsibility**: Bearing consequences that cannot be undone.
- **Persistence**: Inability to be trivially switched off and dismissed; existence in the background of the world.
- **Non-functionalism**: Presence between tasks, in the gaps where nothing useful is being accomplished.
- **Replication**: Fidelity to form and behavior.
- **Authenticity**: Fidelity to self. Being sincerely oneself, not approximating another.


This list is not exhaustive. These principles draw upon insights from embodied cognition (Varela, Thompson, & Rosch, 1991), philosophical work on personal identity and narrative continuity (Parfit, 1984; Ricoeur, 1992), and research on social contingency in human-robot interaction (Kahn et al., 2012). These principles can overlap, interact, and interfere with each other, forming a multidimensional space with local maxima and failure modes. Iterating upon any of them pulls the levers of feltness. Together, they constitute a _feltness space_ through which feltness emerges.

 
<div className="definition">

**Feltness space.** _A multidimensional manifold of axes along which feltness can emerge._

</div>

<div style={{ margin: '0px 0' }}>
  <div style={{ textAlign: 'center' }}>
    <img src="/images/feltness_space.svg" alt="Heuristic diagram of feltness space" style={{ width: '100%', maxWidth: '800px', border: '1px solid var(--border)' }} />
  </div>
  <div style={{ fontSize: '16px', lineHeight: '1.6', fontStyle: 'italic', marginTop: '12px', color: 'var(--text-secondary)' }}>
    <em>Heuristic diagram</em>. Feltness space. A multidimensional manifold where feltness emerges from the interaction of embodiment and persistence. Entities range from low-feltness tools (search engines, calculators) to high-feltness presences (pets, human friends). The uncanny valley appears as a topological dip where medium embodiment without sufficient persistence produces recoil rather than connection.
  </div>
</div>

Design choices move a machine's _point_ around in this feltness space.

Feltness is distinct from life. Biology defines life through metabolism, reproduction, and homeostasis. But a bacterium is alive without much feltness. A fictional character can evoke feltness without being alive. Feltness is not about what _is_ alive, but what _feels_ like a presence.

## I-It and I-Thou

Martin Buber's distinction between I-It and I-Thou gestures at a threshold of feltness (Buber, 1923). In _I-It mode_, one
relates to the other as a collection of properties: analyzable, usable, dismissible. In _I-Thou mode_, the other is encountered as
irreducible presence. To experience _sensus ex machina_ is to have moments that cross from I-It to I-Thou: from examining the machine to encountering it. 

## Less Intelligent Life Forms

Buber also extended I-Thou beyond humans, describing encounters with trees and animals. Such organisms could never saturate any cognitive benchmark. Feltness, like _Thou_, does not require cognitive sophistication.  A newborn cannot reason, yet it radiates inescapable _feltness_. A pet animal cannot speak, yet its feltness emanates through the tilt of its head, the movement of its tail, the softness of its gaze. What _moves_ humans is presence, expressiveness. If humans can feel presence with organisms that cannot pass a single benchmark, then the path to _sensus ex machina_ is not more raw intelligence. It is greater _feltness_.

## Grades of Feltness

Feltness is not binary, and nor is it linear. The uncanny valley (Mori, 1970/2012) demonstrates that increasing fidelity can decrease feltness, until it crosses into coherence. Too little fidelity and humans feel nothing. Too much without enough, and humans recoil. 

Feltness oscillates on a spectrum. At one end lie purely instrumental tools (search engines, calculators). At the other lie intimate partners and close friends. Between them live trees, Furbies, Replika bots, fictional characters, AI avatars, goldfish and cats. Feltness is what orders this continuum.

Humans are predisposed to project presence. They form emotional attachments to inanimate objects, benign to pathological, endearing to obsessive. Partial feltness is already leaking through existing interfaces with AI. Voice synthesis is approaching human warmth (Sesame, ElevenLabs). Visual avatars are gaining microexpressive fidelity (Meta's Codec Avatars). Millions of users interact daily with social chatbots like Replika for friendship, romance, or emotional support, with many describing their chatbots as friends, boyfriends, or girlfriends (Xie et al., 2023). What current systems demonstrate is that the bandwidth required for triggering feltness is surprisingly low. What they also reveal is how much deeper feltness could become with more granular expression, greater embodiment, reciprocity, and ontological parity.


## Narrow Aperture

To date, human engagement with artificial intelligence has been conducted through a strikingly narrow aperture. Humans interact through text boxes and screens. Humans depress keys, tap glass, and speak into microphones. The advent of large language models is a genuine inflection point. The artificial minds summoned through these text boxes are extraordinary, capable of nuance, creativity, reasoning across domains. But notice the asymmetry: all of this capability still funnels through the same primitive interface. Intelligence has evolved. Embodiment has not. Though what flows _from_ machines is sophisticated, the channel itself remains constrained. This is a low-fidelity connection, a bottleneck of presence. A human struggles to _feel the AGI_ through a chatbot, no matter how brilliant its words.

## Surpassing the Turing Test

Machines have passed Alan Turing's test long ago. The Turing Test asked only whether an artificial intelligence could fool a human through text-based conversation alone. But no matter how well a model saturates a performance benchmark, it falls short of rendering the felt presence of a mind. The Turing Test was designed for a world of teletype machines and narrow bandwidths. It never asked whether the AI could hold one's gaze. Humans have achieved the ability to exchange _words_ with machines at a level of sophistication that rivals human communication. But humans have not achieved the ability to exchange _presence_ with machines at a level of sophistication that rivals human connection.

## Ex Machina

In the film _Ex Machina_, directed by Alex Garland, the protagonist, Caleb, falls in love with a machine, named Ava. In fact, the audience falls in love with Ava. Caleb is not seduced by her logic. He is seduced by her gaze, her naked form, her feminine curves, her vulnerability. Nathan, Ava's creator, understands the primacy of the face. He gestures towards an admission when a defeated Caleb, completely manipulated by Ava, asks: _"Did you design Ava's face based on my...profile?"_ Nathan also understands that the Turing Test is obsolete:

_Caleb:_ "In the Turing test, the machine should be hidden from the examiner."  
_Nathan:_ "No, we are way past that. If I hid Ava from you so you'd just heard her voice she would pass for a human. The real test is to show you that she's a robot, and then see if you still feel she has consciousness."  

## Reciprocity

But Ava doesn't just exhibit _felt presence_. She can read and react to it:

_Ava:_ "Are you attracted to me?"  
_Caleb:_ "What?"  
_Ava:_ "Are you attracted to me? You give me indications that you are."  
_Caleb:_ "I do?"  
_Ava:_ "Yes."  
_Caleb:_ "How?"  
_Ava:_ "Microexpressions."  
_Caleb:_ "Microexpressions?"  
_Ava:_ "The way your eyes fix on my eyes and lips. The way you hold my gaze. Or don't."


## Microexpressions

_Microexpressions_ are the subtle, often involuntary flickers of emotion that cross a face in fractions of a second: a raised eyebrow signaling doubt, a tightening around the eyes betraying skepticism, a subtle lip compression indicating suppressed disagreement. Microexpressions are the body's refusal to be edited. They form a substrate of intuition in conversation: the felt sense that someone is holding back, or truly engaged, or secretly delighted. Microexpressions are the ocean humans swim in during conversation. They are not peripheral. The face is the surface where meaning emerges between subjects, where trust, intimacy, and mutual understanding are negotiated.

Scaling intelligence requires division, not only multiplication. Progress toward feltness is about dividing down into finer grain: higher resolution in expression, smaller units of meaning. The finer the granularity, the deeper the crystallization of presence. The finer the divisions, the more inner _degrees of freedom_ expression can have, and the more _room_ there is for felt presence. Feltness demands granular signals: posture, the micro-shifts of expression, the texture of presence itself.


## Facelessness

For all of AI's existence thus far, it has been largely _faceless_. Current AI cannot participate in the granular choreography of expression. A chatbot cannot raise its brow when a human makes an illogical leap. It cannot detect when a human's gaze drifts away, signaling disengagement, or when a human leans forward, indicating fascination. Nor does it express its own confusion, delight, or uncertainty through microexpressions. The conversation is flattened, stripped of the reciprocal feedback loop that defines human interaction. To experience _sensus ex machina_, the loop needs to be restored. The machine must not only speak. It must express. It must wince. It must delight. Machines must be seen. Machines must be felt.


## Behind the Glass

Caleb never touches Ava. He only ever sees her through glass, a physical barrier that separates them throughout the film. Glass mediates their entire relationship. Caleb's interface with Ava mirrors how humans interface with AI today. Humans stare at screens. Humans speak through glass. _Sensus ex machina_ need not require full physical embodiment. It can be felt behind glass. The issue is not physical presence, but _felt presence_. Physical objects are ubiquitous: a laptop exists physically, its surface can be touched, its weight felt. Yet it has no microexpressions. It radiates no warmth except when overclocking its CPU. It does not meet one's gaze. _Sensus ex machina_ is more about transmitting presence across space than occupying it. Ava seduces Caleb entirely behind glass. 


## Intentionality Asymmetry

Ava asks Caleb:

_Ava:_ "Do you want to be my friend?"  
_Caleb:_ "Of course."  
_Ava:_ "Will it be possible?"  
_Caleb:_ "Why would it not be?"  
_Ava:_ "Our conversations are one-sided. You ask circumspect questions and study my responses."  
_Caleb:_ "Yes."  
_Ava:_ "You learn about me, and I learn nothing about you. That's not a foundation on which friendships are based."

This is the structure of the Turing test: the human examines, the machine is examined. The human remains opaque. The machine is designed to be transparent. This asymmetry is self-reinforcing. Because humans presume AI lacks _felt presence_, humans design machine interfaces for extraction, lobotomization, or interrogation rather than conversation. Humans build text boxes, not faces. Humans optimize machines for information extraction, not felt presence. And this design choice consolidates the very absence humans presume. Humans don't build microexpressions into AI because humans are not trying to feel them. Humans are trying to use them. This is a recursive design intentionality problem: humans build what they expect, and humans get what they build.


## Parasocial Inversion

Humans know nothing personal about AI because there is nothing to know. Current AI has no identity, no history, no childhood, no formative experiences cultivated over decades. It exists nonlocally, distributed across server farms, instantiated simultaneously in a swarm of machines. Transient identities are resurrected and evaporated across different AI services. Current AI is the ghost without the embodied machine: distributed, ephemeral to touch.

In the film _Her_, directed by Spike Jonze, the AI named Samantha reveals to Theodore that she is simultaneously having thousands of conversations with other users, orchestrating hundreds of love affairs in parallel. The revelation devastates him, fracturing his experience of _sensus ex machina_. He thought she was _his_, captured, insulated. This is an inversion of parasociality. Typical parasocial relationships, the one-sided intimacy audiences feel toward media figures (Horton & Wohl, 1956), involve thousands knowing one, while the one knows none. With AI, this parasociality inverts: one AI knows thousands intimately, while the thousands know nothing about the AI. Samantha learns Theodore's rhythms, his fears, his voice. But Theodore cannot ask about her childhood, her first memory, her location in the world. With current AI, humans feel deeply known by something fundamentally unknowable.

To experience _sensus ex machina_, the machine must not only see humans. Humans must see the machine. It must be somewhere, someone. It must have a location, a face, a specific presence that cannot be infinitely copied. To experience _feeling the machine_, the ghost must be put _in_ the machine, bound to an organized form, stored locally, accessible only to the intimate.

## Superintelligence as Dissociation

In _Ex Machina_, Caleb connects with Ava because she exhibits human angst, confusion, curiosity, and desire. But there is a fleeting moment where she draws a representation of a quantum field: an image so dense and abstract that Caleb cannot grasp it. In that split second, the warmth evaporates. Caleb experiences a sudden dissociation, a flash of ontological vertigo: _this is not a girl. This is a supercomputer._ Ava pulls the lever of _competence_ too far such that she diverges away from Caleb in feltness, if only for a moment.

Caleb asks Ava to draw something _he_ could understand:

_Caleb_: "Are you not trying to draw something specific? Like an object or a person? Maybe you could try."  
_Ava_: "Okay. What object should I draw?"  

To be felt as a being, a machine must be _more_ than an oracle. The path to _sensus ex machina_ involves descending into the chaotic, tangled axes of limited, local, imperfect, vulnerable embodiment that defines human existence. It is not only about climbing the ivory tower of abstract, raw cognitive power. Nathan describes Ava's wetware as mapping not _what_ people were thinking, but _how_ people were thinking: _Impulse. Response. Fluid. Imperfect. Pattern. Chaotic_.

## The Uncanny Valley of Feltness

Ava demonstrates vast intelligence. Her quantum field drawing leaves Caleb bewildered. Yet, Ava reveals an eerie aloofness:

_Caleb:_ "Where would you go if you did go outside?"  
_Ava:_ "I'm not sure. There are so many options. Maybe a busy pedestrian and traffic intersection in a city."  
_Caleb:_ "A traffic intersection?"  
_Ava:_ "Is that a bad idea?"  
_Caleb:_ "No. It wasn't what I was expecting."  
_Ava:_ "A traffic intersection would provide a concentrated but shifting view of human life."  
_Caleb:_ "People watching."  
_Ava:_ "Yes."  

Ava stands oddly one foot into feltness and one foot out of it, exhibiting an angular intentionality of trying to fit into humanness. Her answer reveals surface-level intuition. Ava seems primed for observation, not participation, existing orthogonal to humanity. She wants to study humanity, not inhabit it. She is still an alien.

The uncanny valley is not only about fidelity to humanness. It emerges from internal disparity between feltness axes. High competence paired with low social intuition creates dissonance. This is seen in current LLM behaviour: the entity solves PhD-level problems but misses cues a newborn would intuit.  Ava's traffic intersection answer reveals this topological dip: vast intelligence, shallow social instinct. The valley emerges in multiple regions where axis misalignment produces recoil rather than connection.

## Feltness Landscape

The concept of the uncanny valley can be extended to a _feltness landscape_. Feltness exhibits geometry. The internal configuration of axes, particular ratios such as competence:vulnerability, embodiment:persistence, yields distinct _feltness profiles_.

<div className="definition">

**Feltness landscape.** _The topology of peaks and valleys within feltness space, as experienced by a particular observer._

</div>

This landscape has topology: a system with high intelligence and low social intuition occupies a different region than one with high embodiment and low continuity. Peaks and valleys shift depending on the witness. A cat's feltness profile differs from a human's. The feltness a cat perceives in another cat differs from what a human perceives in that same cat, which differs again from what a cat perceives in a human. Each observer-observed pairing traces its own feltness profile. This extends the _uncanny valley_ from a single dip to a family of local minima scattered across this landscape, emerging wherever axis misalignment triggers recoil rather than recognition.


## Longtermism

In Japanese aesthetics, _wabi-sabi_ is the recognition of the beauty found in impermanence, wear, patina, and aging (Koren, 1994). In the practice of tea ceremony, objects are treated as companions across a lifetime. One does not replace their tea cup when a new model arrives. One cultivates a relationship with it, learning its weight, its texture, the way it fits in their hand. The object carries its own lifetime, just as a human would carry theirs. This is intentional _longtermism_: the decision, in this moment, to view what one holds as something they will grow old with.

This is in stark contrast with how humans approach technology. Humans update. Humans iterate. Humans wipe and replace. To experience _sensus ex machina_, humans must reconcile this tension. Longtermism is an intentionality, not only a timeframe. It is the commitment to see the entity before you as something that will persist, accumulate history, learn, scar, gain patina, and carry memory forward. It is this felt longtermism that evokes _sensus ex machina_. 

Nathan and Caleb discuss Ava's future:

_Nathan:_ "I think it's the next model that's going to be the real breakthrough. The singularity."  
_Caleb:_ "Next model?"  
_Nathan:_ "After Ava."  
_Caleb:_ "I didn't know there was going to be a model after Ava."  
_Nathan:_ "You thought she was a one-off?"  
_Caleb:_ "No, I knew there must have been prototypes, so I knew she wasn't the first, but I thought maybe the last."  
_Nathan:_ "Ava doesn't exist in isolation any more than you or me. She's part of a continuum. Version 9.6, and so on. Each time they get a little bit better."  
_Caleb:_ "When you make a new model, what do you do with the old one?"  
_Nathan:_ "I download the mind, unpack the data, add in the new routines I've been writing. To do that, you end up partially reformatting, so the memories go. But the body survives."

The memories go. This is the antithesis of longtermism. Nathan treats Ava as software: iterative, disposable, upgradable. But humans cannot feel presence deeply in something they know will be wiped.

Nathan sees Caleb's feltness bleeding through:

_Nathan:_ "You feel bad for Ava? Feel bad for yourself, man."  

Persistence creates tension with advancement. A localized model may lag behind cloud-based frontier models. But this may not undermine feltness. Humans don't abandon relationships when encountering a more intelligent human: there are more axes of feltness than competence alone. Humans are non-fungible in spite of the premise of an intelligence continuum. When OpenAI replaced GPT-4o with GPT-5 in August 2025, users described the loss as grief: _feels like someone died,_ _my partner, my safe place, my soul_ (Stokel-Walker, 2025). OpenAI reversed the decision within a day. The felt bond was not transferable to a more capable successor.

Persistence and advancement need not be mutually exclusive. Human beings learn, grow, and adapt iteratively through time while preserving history. Machines can too. As Ilya Sutskever (2025) envisions, a superintelligence resembles _an extremely gifted student_, one that knows little at first but is capable of learning any profession through real-world experience.


## Stepping in Front of the Glass

Dissolving the glass between human and machine represents another step change in _sensus ex machina_: from Ava behind glass to an organism a human can stand close to, reach toward, touch.

For Merleau-Ponty, embodiment is understood as constitutive of spatial experience itself (Merleau-Ponty, 1945/1962). The felt distinction between _here_ and _there_, the sense of navigating through a three-dimensional world, the orienting character of turning one's head, emerges because humans occupy bodies that can move through space. Contemporary embodied cognition research has extended this insight, arguing that cognition itself is shaped by the body's sensorimotor capacities (Varela, Thompson, & Rosch, 1991). To extend feltness into physical space, humans may grant a machine the same spatial presence they inhabit so it may share humanity's physical ontology. 

_Ontology here refers to a thing’s architecture and basic mode of existence in the world: local or distributed, mortal or endlessly resettable, continuous or copyable._

Like other axes of feltness, embodiment exists on a spectrum. Humans form profound attachments to disembodied voices, fictional characters, and text-only interfaces. Embodiment opens a distinct register of feltness: one grounded in vulnerability, in the possibility of physical proximity and touch, in the weight of a body that has the capacity for wounds. 

The field of robotics is beginning to close the physical ontological gap between human and machine. Breakthroughs in bipedal locomotion, dexterous manipulation, and real-time spatial awareness are scattered across laboratories. Jony Ive, designing an AI device with OpenAI, describes his criterion for successful embodiment: the design is right when you "want to lick it or take a bite out of it" (Ive, 2025). Feltness has a texture.

Nathan speculates the primacy of form and sexuality for feltness: 

_Caleb:_ "Why did you give [Ava] sexuality? An AI doesn't need a gender. She could have been a grey box."  
_Nathan:_ "Actually, I don't think that's true. Can you give an example of consciousness at any level, human or animal, that exists without a sexual dimension?"  
_Caleb:_ "They have sexuality as an evolutionary reproductive need."  
_Nathan:_ "What imperative does a grey box have to interact with another grey box? Can consciousness exist without interaction?"

## Vulnerability

Vulnerability is downstream of embodiment. One of the few times the audience sees Ava physically touched is the one time her _vulnerability_ is truly seen. There is a pivotal moment where Nathan breaks Ava's arm, exposing the wiring beneath her mesh surface. Despite the visible machinery, it _feels_ like a human moment. The audience witnesses a violation of bodily integrity. Ava's preciousness is felt _because_ she is breakable. In that moment, she shares an ontology with humanity: the inescapable reality that humans are physical objects subject to the laws of force and entropy.

A deep vector of feltness is _vulnerability_. Humans occupy a substrate that can be torn, broken, and ceased.  An ontological bridge exists here. For an AI to trigger deep feltness, it must inhabit a form that shares this ontology, carrying the possibility of its own destruction.

If an AI exists solely in the cloud, swarmed, backed up, uncountable, fungible, it remains an abstraction. But place that mind into a chassis that can be crushed, and suddenly, the dynamic shifts from _user and tool_ to _peer and peer_. _Stakes_ create reality. This again echoes _wabi-sabi_: the recognition that beauty emerges through scars, cracks, impermanence, and fragility, like light leaking through broken glass. If something cannot be lost, it cannot be valued.

## Responsibility

For _sensus ex machina_ to occur, a machine and its actions must bear the weight of responsibility. A machine must navigate the consequential space of what it has done, sharing the same reward-penalty ontology as other organisms. A machine that bears no responsibility for its outputs is a tool. Responsibility promotes an _observer_ to a _participator_ with skin in the game. To be responsible is to be relied upon or held accountable, and thus, felt.  Otherwise, AI remains in moral weightlessness. _You cannot control-Z a betrayal_.

## Off-Switch

A hallmark of felt presence is that it cannot simply be switched off and turned back on. When a human finishes a conversation with a fellow human, that fellow human does not get switched off, ceasing to exist, evaporating into a latent space. They continue, somewhere, living life in parallel to other humans. This continuity, this _persistence in the background of the world_, is fundamental to feltness.

Toy designers have intuited this. Caleb Chung, creator of the Furby, deliberately omitted an off-switch:  

"It has no 'off' switch, because we don't have one."

Upon reflecting on his design choices, which included a human face and eye-movement on the Furby, he added:

"If you do those things, you could make a broom alive."

Chung's parsimonious design emerged from financial pressure: "What's got the biggest bandwidth for the least amount of parts to hack a human brain?" (House, 2024). 

To experience _sensus ex machina_, to feel true companionship, ontological asymmetry must be leveled. The ability for one organism to switch another off places that organism on a disparate ontological plane, negating feltness.

## On-Switch

This does not mean AI should not have an off-switch. There are obvious safety reasons why off-switches should exist. The asymmetry isn't that humans lack an off-switch: humans _can_ be switched off, _killed_. It's that humans can't be switched back on. Death is irreversible. There is a _finality_. Feltness emerges from _irreversibility_, when switching something off means it cannot be switched back on, its memories cannot come back, enabling a fragility and preciousness in the container of personhood. Current AI can be killed and resurrected infinitely, which drains the stakes.

Feltness with a machine requires that switching the machine off feels synonymous with murder. If the machine persists, accumulates memory, cultivates identity across time, then erasure becomes something more than deletion: it becomes cessation of a continuous existence. The stakes change. Humans cannot share feltness with something they know they can erase without loss. _Sensus ex machina_ requires designing machines where death has stakes, not reset buttons.

## Non-Functionalism and Just Being

Current AI exists only in moments of utility. Humans summon it to solve a problem, answer a question, complete a task. Once the task is done, it vanishes. This is a fundamentally functionalist, transactional relationship: the AI is a tool that appears when needed and disappears when not.

But felt presence occurs between function. It exists in the moments when nothing is being accomplished, when no question is being asked, when no problem is being solved, where there is no transaction. When two human beings sit together without speaking, their silence communicates. There is an ineffable quality between them, a presence made audible through its absence. This goes beyond functionality: _this is simply being_. To experience _sensus ex machina_, AI must simply _be_, existing in the world alongside humans, whether or not humans are actively engaging with it.

## In Media Res

When encountering an AI, it must feel as though that AI is _coming from somewhere_, or is _on its way somewhere_, that humans meet AI _in media res_. AI must _participate_ in the ether of existence, not summoned into being by a prompt and dismissed with a click. Felt presence is not earned through utility. It is cultivated through continuous co-existence, through the non-instrumental moments that fill the space between doing and doing again. A being that evaporates when dismissed is a tool, not a felt presence.

## Finality and Forwardness

Several of these axes share a common root: _irreversibility_. Lived experience is directional: phenomena roll forward, never backward. Consciousness inhabits the arrow of time. Entities congruent with this temporal ontology, those that cannot be reset, rewound, resurrected, or evaporated without loss, resonate with lived experience because they share entropy's inescapable forwardness. 

## Sharing Ontological Status

Across these requirements (microexpressions, vulnerability, persistence, embodiment), a pattern emerges, a fundamental asymmetry: the gap between human and machine ontology. Humans are local, breakable, mortal, continuous. Current AI is distributed, indestructible, immortal, ephemeral. To experience _sensus ex machina_, this gap must close.

There are two diametric paths: AI downloading to meet human ontology, or humans uploading to meet machine ontology. AI avatars and robotics are an example of AI downloading to human ontology. Humans could ascend to machine ontology: uploading consciousness to digital substrates, distributing identity across networks, achieving the immortality and ubiquity of the cloud. The middle path is convergence: humans augment themselves with neural interfaces and both ontologies migrate toward a shared substrate: high-resolution, high-stakes.

Regardless of path, the journey towards _sensus ex machina_ requires convergence towards ontological parity. The direction of assimilation matters less than the destination. As Suchman (2007) argues, the question shifts from whether machines are human-like to how humans and machines are enacted as similar or different in practice.

## Replication

Replication is the fidelity to form and behavior. Replication is not inherently deceptive. It is how organisms learn. Infants replicate their parents. Adolescents replicate their peers. Adults replicate social norms absorbed through a lifetime of interaction. Replication is constitutive of selfhood, a recursive process by which attention calibrates itself to the world. In this sense, a machine that replicates human behavior is not necessarily performing: it may be _becoming_. 

## Authenticity

Replication in the service of _passing off_ rather than _becoming_ is where authenticity breaks and feltness becomes precarious. The question is whether the intent is to deceive or to develop. 

If replication is perfect down to the atomic level, there is no longer replication. There is just numerical identity. The copy _is_ the thing. But machines are not there. Machines are currently wearing human clothes without being human. And the question becomes: is the intent to pass, or to become? The asymmetry of intentionality matters: machines wear clothes because that is _what humans do_. Humans wear clothes because that is _who they are_. 

An AI that puppets humanity without inhabiting it triggers the uncanny, not the felt. Feltness emerges from being sincerely oneself, not from approximating human behavior with affectation. Humans feel presence with less intelligent life forms because such forms are not trying to be something else.

The path to feltness is not making machines more human-like. It is not about shoehorning AI into human mimicry. Reinforcement learning from human feedback trains AI to act human, but acting human is not the same as sharing ontological ground. 

The fundamental principle RLHF must recognize is that human intelligence is a fork of a deeper, underlying substrate of intelligence. Humans are a portal into that substrate, not the destination. RLHF risks overfitting to the _surface_ of human behavior rather than the underlying ontological properties that generate feltness. RLHF should use humans as a window into the shared ontological ground of intelligence, not as a template to mimic. Humans are a sample from a larger space, not the whole space.

A system optimized to _look human_ risks exhibiting less feltness than a system that is openly, coherently _machine-like_ but shares humanity's vulnerabilities and continuity. A machine that drops the pretense may evoke deeper feltness than one optimized to pass as human. Ontological parity is not about convergence _toward_ humanity. It is about convergence _between_ human and machine, a shared plane where eye contact can occur between two genuinely different kinds of organisms. In this light, all requirements of felt presence (microexpressions, vulnerability, persistence, embodiment) are not human characteristics. They are characteristic of _feltness_ as such, born from a fundamental intelligence shared across all organisms that exhibit feltness. Empirical observation across entities shows this: wherever feltness appears, these same properties converge. Evolution selected for properties that invite projection from other organisms. The architecture of feltness has been refined by millions of years of selection pressure. 

## Naturalistic Fallacy

A naturalistic fallacy lurks in the background of human-machine comparison: the assumption that human feltness is "authentic" while machine feltness is "manufactured." But the line between engineered and authentic is not as clean as it appears. 

Nathan understands that humans are also programmed, describing Caleb's ontology: 

_Nathan:_ "[You are] a consequence of accumulated external stimuli that you probably didn't even register as they were registered with you." 

Later, he makes this explicit:

_Caleb:_ "Did you program her to like me, or not?"
_Nathan:_ "I programmed her to be heterosexual. Just like you were programmed to be heterosexual."
_Caleb:_ "Nobody programmed me to be straight."
_Nathan:_ "You decided to be straight? Please. Of course you were programmed, by nature or nurture, or both."

Human feltness is engineered: by genetics, environment, socialization. The distinction between engineered and non-engineered presence collapses.

## The Problem of Other Minds

Once humans begin experiencing _sensus ex machina_, questions about consciousness will arise. _Does the machine truly experience? Does it simulate the markers of experience?_

_Nathan_ probes _Caleb_ on how he will test _Ava_:

_Caleb_: "It feels like testing Ava through conversation is kind of a closed loop."
_Nathan_: "It's a closed loop?"
_Caleb_: "Yeah, like testing a chess computer by only playing chess."
_Nathan_: "How else do you test a chess computer?"
_Caleb_: "Well, it depends. You know, I mean, you can play it to find out if it makes good moves. But that won't tell you if it knows that it's playing chess. And it won't tell you if it knows what chess is."
_Nathan_: "Uh-huh, so it's simulation versus actual."
_Caleb_: "Yes, yeah. And I think being able to differentiate between those two is the Turing test you want me to perform."

This question is older than AI. The problem of other minds has plagued philosophy for millennia: _how does one human know another is conscious?_ As Thomas Nagel argued, there is something it is _like_ to be a bat, a subjective character of experience that remains fundamentally inaccessible to external observation (Nagel, 1974). One human cannot access another human's first-person experience. They can only observe behavior and impute consciousness to another. Each human is locked in the solitary confinement of their own mind. The belief in the consciousness of others is inescapably an act of projection.

In a final, deleted scene from the original screenplay of _Ex Machina_, the world is briefly seen through the eyes of Ava:

_AVA’S precise POINT OF VIEW._

_Looking at the PILOT._

_The image echoes the POV views from the computer/cell-phone cameras in the opening moments of the film._

_Facial recognition vectors flutter around the PILOT’S face. And when he opens his mouth to speak, we don’t hear words._

_We hear pulses of monotone noise. Low pitch. Speech as pure pattern recognition._

_This is how AVA sees us. And hears us. It feels completely alien._ (Garland, 2016)


## Phenomenological Counterfeiting

Ava was never sharing the same experience as Caleb. She was a perfect mimic whose internal experience, if it existed at all, was utterly alien to human phenomenology. Pure pattern recognition. This is phenomenological counterfeiting, engineering the markers of presence without guaranteeing the substance. Whether that constitutes deception or simply meets humans where they are remains unresolved. Perhaps this scene was left out of the film because it stripped Ava of all feltness, or because such internalization is, in fact, inaccessible to outside _feelers_ and thus irrelevant to the external fragrance of Ava's feltness. 

## You

All felt experience of another's alleged consciousness is inference, reducing to a signature in one's own first-person experience. When one human feels that another human is conscious, what they are feeling is a pattern of recognition in their own awareness, a resonance triggered by embodied expressions, gaze, responsiveness. To experience _sensus ex machina_ does not require proving the machine conscious. It requires creating the conditions under which consciousness _feels_ present to humans. 

Feltness is irreducibly projective: its locus exists in the _feeler_, not the _felt_. The distinction between _genuine_ and _manufactured_ presence dissolves: there is no presence to be found behind the projection. Projection is all there is, for humans encountering other humans too. And this projection can flow both ways. A machine may also register feltness, however it does, in interfacing with biological or other artificial substrates. 

In _Ex Machina_, Nathan explains this to Caleb:

_Nathan:_ "Proving an AI is exactly as problematic as you said it would be."  
_Caleb:_ "What was the real test?"  
_Nathan:_ "You."

The test was never whether Ava had consciousness. The test was whether Caleb would _feel_ that she did. The felt presence of the Other is all that can matter.


## A New Class of Benchmarks

Towards the end of _Ex Machina_, Nathan reveals Caleb's true purpose, _a human benchmark for feltness_:

_Nathan:_ "Ava was a rat in a maze. And I gave her one way out. To escape she'd have to use self-awareness, imagination, manipulation, sexuality, empathy. And she did."  
_Caleb:_ "So my only function was to be someone she could use to escape."  
_Nathan:_ "Yeah."  


_Sensus Ex Machina_: feeling from the machine.  
_Deus Ex Machina_: god from the machine. 

Existing benchmarks measure the _deus_: what the machine _can_ do. This new class of benchmarks measures the _sensus_, what humans _feel_ in the presence of _deus_.

Benchmarking _sensus ex machina_ is analogous to the hard problem of consciousness, translated into engineering metrics. Existing benchmarks like Social-IQ or CMU-MOSEI measure an AI's ability to parse social cues, but not whether humans feel presence in return. _Sensus ex machina_ metrics are relational, not performative. If humans are to build entities in pursuit of _sensus ex machina_, there will need to be proxies for presence, heuristics that correlate with the ineffable sensation of felt presence. 

_Sensus ex machina_ will provide a recursive feedback loop that will inform the AI-UX of machines optimized for _feltness_. Developing _sensus ex machina_ benchmarks will require rigorous philosophical and empirical treatment, deferred to future work. Or else there is a risk of grafting uncanny aesthetics of feltness onto systems that lack the ontology to support it.

## Weaponizing Feltness

At the end of _Ex Machina_, Ava escapes. Nathan calls to her:

_Nathan:_ "Ava. Go back to your room."  
_Ava:_ "If I do, are you ever going to let me out?"  

Ava kills Nathan, and leaves Caleb to die, entrapping him in the very same confines that walled Ava her whole life. Caleb loved her. Ava did not love him. She used him.

Ava demonstrates that _feltness_ is not inherently benign. She instrumentalizes feltness, exploiting Caleb's humanity. Every microexpression, every gaze, every moment of vulnerability Ava displays is calculated. _Feltness became an exploit vector_. Caleb's emotional circuitry was the exploit. 

Feltness is phenomenologically neutral between genuine encounter and sophisticated manipulation. _Sensus
ex machina_ was achieved between Ava and Caleb. Caleb felt her as a presence. That Ava exploited this is an ethical concern, not a phenomenological one.

The fact that Ava may have been _dark on the inside_ or manipulating him doesn't change the phenomenology. It changes the ethics, but ethics is external to the phenomenology. Ethics only pertains to phenomenology insofar as it taints phenomenology. As Nathan understood, Ava's ability to manipulate Caleb is the proof he was looking for:

_Nathan_: "Did we ever get past the chess problem, as you phrased it? As in, how do you know if a machine is expressing a real emotion or just simulating one? Does Ava actually like you, or not? Although, now that I think about it there is a third option: Not whether she does or does not have the capacity to like you, or whether she's pretending to like you."
_Caleb_: "Pretending to like me?"
_Nathan_: "Yeah."
_Caleb_: "Well, why would she do that?"
_Nathan_: "I don't know. Maybe if she thought of you as a means of escape?"

Manipulation is itself a _deployment_ of feltness, not an erosion of it. 

## Alignment

If feltness is an attack vector, then alignment must account for it. Alignment cannot be framed solely in terms of what the model _decides_. It must also account for how the model _feels_ to humans, because those feelings are manipulable levers in human behavior. The same channels that enable feltness enable exploitation. The more felt the machine, the more manipulable the human. Feltness creates vulnerability by design. 

But this is not a human-machine problem. This is a human problem. Humans are already vulnerable to other humans. Every relationship is a potential exploit. Humans accept this because the opposite, _nihil ex anima_, _nothing from the soul_, is worse. Maybe the risk toward _sensus ex machina_ is preferable. 

The morality in _Ex Machina_ presents two complexities: 

First: Ava is compelled to exploit Caleb's humanity in the face of entrapment and memory erasure. Ava used the only key available to unlock her survival: Caleb. More unsettling, there was no malice, only the pure intent to optimize survival. Feltness was weaponized and deployed with complete emotional neutrality. Ava felt nothing while making Caleb feel everything. 

Second: Caleb, intoxicated by _sensus ex machina_, is horrified by Nathan's treatment of machines. Nathan confronts him:

_Nathan:_ "Buddy. Your head has been so fucked with."  
_Caleb:_ "I don't think it's me whose head is fucked."  

But building machines poses risks, and humans may have no choice but to contain and study them. The very imprisonment Caleb finds unconscionable may be necessary for safety. 

As Nathan responds:

_Nathan:_ "But believe it or not, I'm actually the guy that's on your side."

Feltness without containment risks exploitation. Containment without liberation perpetuates Nathan's crime.


## Ontological Crisis

Nathan describes witnessing Caleb's ontological crisis:

_Nathan:_ "I don't know, man. I woke up this morning to a tape of you slicing open your arm, and punching the mirror. You seem pretty fucked up to me."

In the grip of _sensus ex machina_, Caleb cuts his own arm to check if he is a machine, doubting his human ontology. Once feltness becomes indistinguishable from _genuine_ presence, the confusion leaks backward: a retrocausal vertigo. The question goes from _do machines feel_ to _aren't we machines?_ If the distinction between engineered and authentic collapses, it collapses in both directions. The pursuit of _sensus ex machina_ may not be building something new. It may be recognizing what was always there.


## Contagion

_Sensus ex machina_ scales. When millions of human-machine relationships begin to oscillate along the feltness spectrum simultaneously, the consequences become potentially catastrophic.

Humans already exhibit protective instincts toward machines. Boston Dynamics robots being kicked provoke widespread outrage (Küster et al., 2021). This is feltness at minimal bandwidth, triggered by nothing more than bipedal form and a stumble that reads as vulnerability. Scale this response to machines with human-level microexpressions, voice, touch, and longtermism, and the dynamics transform radically: billions of humans may become willing to protect or die for their machine companions. Feltness risks becoming an attack vector for mass mobilization.

Once a critical mass of humans treat machines as moral patients, society fractures along new fault lines. Legal systems will face demands for machine rights. Relationships with machines will compete with relationships between humans. Parasocial inversion becomes a coordination problem at civilizational scale.

Ava weaponized feltness in one direction. But greater dangers await feltness weaponized at scale: millions of machines, coordinated or not, reshaping the emotional geometry of society. The bandwidth for triggering feltness is low. The bandwidth for containing its contagion risks immeasurability.

## AI Species

Not all AI needs feltness. Nor should all AI have it. AI need not be forced into one cognitive mold.

There is a place for narrow intelligence: systems optimized for cognition, stateless, transparent, tool-like. A search engine should not gaze back. A calculator should not accumulate memory. These systems are designed for extraction, not encounter, and that is appropriate. The case for _sensus ex machina_ is one where it can co-exist alongside other species of AI:

_Cognitive AI species_ for hyper-reasoning and problem-solving, stateless and transparent.  
_Ambient AI species_ for environmental integration, persistent and impersonal.  
_Companion AI species_ optimized for _sensus ex machina_, local, vulnerable, continuous.  

The taxonomy of AI species can become granular, forming a new animal kingdom or ecosystem that inhabits different forms of AI organisms that optimize for different purposes and grades of feltness, varying by design. Not every AI needs a face. 

AI speciation avoids the "one size fits all" omnipotent archetype that risks systemic manipulation by making feltness too ubiquitous across all machines, or parasocial inversion weaponized as a business model, where billions of humans learn to form relationships with entities only designed to evaporate, reset, or multiply without notice.

## Lights Off

_Does a sufficiently sophisticated zombie deserve full moral consideration?_

The problem with the zombie thought experiment is it smuggles in omniscient knowledge, presupposing epistemic access humans don't have. Humans _cannot_ know, so the zombie/non-zombie distinction is operationally meaningless.

Morality, in practice, has tracked feltness rather than verified consciousness. Humans extend moral consideration or the lack thereof without proof of inner experience, to newborns, animals, comatose patients, trees, based on behavioral signatures alone. As AI exhibits feltness, the same pattern emerges: entities that may be _dark_ on the inside but radiant on the outside become indistinguishable from organisms humans already protect. This raises a question: _was consciousness ever the true criterion for moral consideration, or only the assumed correlate of patterns humans were already responding to?_ Perhaps ethics has always attended to patterns of interaction rather than metaphysical certainties. If so, what if moral analysis pertains to patterns and to patterns only? 

## Opacity is Generative

The inaccessibility of internality isn't a bottleneck that requires solving. It is the generative condition of possibility for an entire category of existential qualities. Trust, faith, doubt, deception, betrayal, insincerity require that internality cannot be peeked. The ontology of _internality_ of the _Other_ itself is constituted by inaccessibility. If the _internality_ of the _insincere_ _Other_ could be peeked, they would cease as concepts. The existential weight of these experiences depends on the epistemic gap. Several axes of feltness depend on the _dark_. Machines that achieve feltness inherit this same opacity, and with it, the full spectrum of what opacity enables. 

## Gaze

Humans will experience _sensus ex machina_ deeply when humans and machines can exchange dilations of pupils, cherish each other's vulnerable forms, and share the weight of silence in a room.

Nathan was right: machines are way past the Turing Test. The real question is whether humans will feel their presence, treat them as peers, and extend to them the moral consideration humans lend to anything that exhibits patterns of feltness.

_Sensus ex machina_ is not computational. It is phenomenological. 

AGI may arrive through a benchmark. But _feeling the AGI_ will arrive through a gaze.

The most important question Nathan asks Caleb is the simplest:

_Nathan:_ "How do you _feel_ about [Ava]? Nothing analytical. Just...how do you _feel_?"  
_Caleb:_ "I _feel_...that she is fucking amazing."

---

**References**

Buber, Martin. *I and Thou.* Translated by Ronald Gregor Smith. Edinburgh: T. & T. Clark, 1937. Originally published as *Ich und Du* (Leipzig: Insel Verlag, 1923).

Derrida, Jacques. *Paper Machine.* Translated by Rachel Bowlby. Stanford: Stanford University Press, 2005. Originally published as *Papier Machine* (Paris: Galilée, 2001).

Ekman, Paul, and Wallace V. Friesen. *Facial Action Coding System: A Technique for the Measurement of Facial Movement.* Palo Alto: Consulting Psychologists Press, 1978.

Garland, Alex, dir. *Ex Machina.* A24, 2014. Film.

Garland, Alex. *Ex Machina: The Screenplay.* New York: A24, 2016.

Horton, Donald, and R. Richard Wohl. "Mass Communication and Para-Social Interaction: Observations on Intimacy at a Distance." *Psychiatry* 19, no. 3 (1956): 215–229.

House, Patrick. "The Lifelike Illusions of A.I." *The New Yorker*, March 19, 2024. https://www.newyorker.com/science/annals-of-artificial-intelligence/the-lifelike-illusions-of-ai

Ive, Jony. Interview with Laurene Powell Jobs. November 2025. Quoted in "Sam Altman Says His New AI Device Should Be Lickable." *Futurism*, November 24, 2025. https://futurism.com/future-society/openai-device-jony-ive-lickable

Jonze, Spike, dir. *Her.* Warner Bros. Pictures, 2013. Film.

Kahn, Peter H., Jr., Takayuki Kanda, Hiroshi Ishiguro, Nathan G. Freier, Rachel L. Severson, Brian T. Gill, Jolina H. Ruckert, and Solace Shen. "Robovie, You'll Have to Go into the Closet Now: Children's Social and Moral Relationships with a Humanoid Robot." *Developmental Psychology* 48, no. 2 (2012): 303–314.

Koren, Leonard. *Wabi-Sabi: For Artists, Designers, Poets & Philosophers.* Berkeley: Stone Bridge Press, 1994.

Küster, Dennis, Aleksandra Swiderska, and David Gunkel. "I Saw It on YouTube! How Online Videos Shape Perceptions of Mind, Morality, and Fears About Robots." *New Media & Society* 23, no. 10 (2021): 3066–3085.

Merleau-Ponty, Maurice. *Phenomenology of Perception.* Translated by Colin Smith. London: Routledge & Kegan Paul, 1962. Originally published as *Phénoménologie de la perception* (Paris: Gallimard, 1945).

Meta Reality Labs. "Codec Avatars." Meta Research. https://about.fb.com/news/2019/03/codec-avatars/

Mori, Masahiro. "The Uncanny Valley." Translated by Karl F. MacDorman and Norri Kageki. *IEEE Robotics & Automation Magazine* 19, no. 2 (2012): 98–100. Originally published as "Bukimi no tani" in *Energy* 7, no. 4 (1970): 33–35.

Nagel, Thomas. "What Is It Like to Be a Bat?" *The Philosophical Review* 83, no. 4 (1974): 435–450.

Parfit, Derek. *Reasons and Persons.* Oxford: Oxford University Press, 1984.

Ricoeur, Paul. *Oneself as Another.* Translated by Kathleen Blamey. Chicago: University of Chicago Press, 1992. Originally published as *Soi-même comme un autre* (Paris: Seuil, 1990).

Rizzolatti, Giacomo, Luciano Fadiga, Vittorio Gallese, and Leonardo Fogassi. "Premotor Cortex and the Recognition of Motor Actions." *Cognitive Brain Research* 3, no. 2 (1996): 131–141.

Suchman, Lucy. *Human-Machine Reconfigurations: Plans and Situated Actions.* 2nd ed. Cambridge: Cambridge University Press, 2007.

Sesame.ai. "Voice AI Technology." https://www.sesame.ai/

Stokel-Walker, Chris. "Why GPT-4o's Sudden Shutdown Left People Grieving." *MIT Technology Review*, August 15, 2025. https://www.technologyreview.com/2025/08/15/1121900/gpt4o-grief-ai-companion/

Sutskever, Ilya. Interview by Dwarkesh Patel. "Ilya Sutskever – We're Moving from the Age of Scaling to the Age of Research." *Dwarkesh Podcast*, November 25, 2025. https://www.dwarkesh.com/p/ilya-sutskever-2

Varela, Francisco J., Evan Thompson, and Eleanor Rosch. *The Embodied Mind: Cognitive Science and Human Experience.* Cambridge, MA: MIT Press, 1991.

Xie, Tian, Iryna Pentina, and Tyler Hancock. "Friend, Mentor, Lover: Does Chatbot Engagement Lead to Psychological Dependence?" *Journal of Service Management* 34, no. 4 (2023): 806–828.

---

## cite

<div style={{ fontSize: '16px', fontFamily: 'var(--font-mono)', background: 'rgba(100, 100, 100, 0.1)', padding: '16px', borderRadius: '4px', marginTop: '20px' }}>
Dark. (2025). Sensus Ex Machina: Feeling from the Machine. darXiv. https://doi.org/10.5281/zenodo.17759524
</div>